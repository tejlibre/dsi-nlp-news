{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcdD7pIgSQIo"
   },
   "source": [
    "#1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report details the work done by Scoop finders (Team C) for the [Africa Data Science Intensive (DSI) program](http://dsi-program.com/) Module 3 task. The goal of the task was to come up with an interesting natural language processing (NLP) project using either audio, text or both. \n",
    "\n",
    "We chose to create an app that provides an overview of one week of current twitter data. We chose to focus on tweets from Kenya, but this can easily be customised for another country or region. Our app is aimed at individuals working in the media industry who would like to supplement information gained from traditional media sources with information about currently treading topics on social media. In this way they would be able to better tailor their content to topics that are currently popular or trending. Our app also provides information on the sentiment and subjectively of the tweets as well as a text generation component that is intended to be used to provide background information or introduction. We have also allowed the user to adjust the date range so that they can analysis tweeter data, for example, on a specific date or over a weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FCe5Z4pSXGl"
   },
   "source": [
    "#2. Pipeline Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8xFJ6Tw-4nb"
   },
   "source": [
    "The project pipeline is depicted in the image below. In the deployed app we used only data obtained from twitter however, other data sources that we investigated include GDELT and RSS feeds, which are discussed in the next section, could be integrated into the pipeline. The pre-processing steps applied to our dataset include lemmatisation and removal of stop words. Sentiment analysis and topic modelling can then be performed on this cleaned dataset. \n",
    "\n",
    "Sentiment analysis provides us with polarity and subjectivity metrics. We are also able to determine the top emotions using sentiment analysis and topic modelling. In addition, topic modelling allows us to create workclouds associated with positive and negative emotions, topic workclouds and a visualisation of the topic clusters. Finally, we use GPT-3 to perform text generation in order to provide the user with background information. \n",
    "\n",
    "<img title=\"pipeline\" alt=\"pipeline\" src=\"pipeline.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbc6iolKSfi4"
   },
   "source": [
    "#3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e-RnoBGWxcr"
   },
   "source": [
    "For this project we tapped into the following data sources:\n",
    "1. Tweets : Top trending tweets were harvested for a week from the twitter API.\n",
    "2. GDELT: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN6oJyU_SpBd"
   },
   "source": [
    "#4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIW_dqXivoWn"
   },
   "source": [
    "Sentiment analysis is the task of determining the emotional value of a given expression in natural language. In this project we explored the Polarity and Subjectivity of the texts using rule based models including TextBlob and Vedar. We settled on TextBlob since it is simple and has the ability to perform subjectivity analysis.\n",
    "\n",
    "Polarity analysis determins whether a word, phrase, or document is positive, negative, or neutral. The scores ranges from -1 as very negative to +1 as very posivite with 0 being neutral. The subjectivity score varies from 0 as objective texts to 1 being more subjected texts. The higly subjective texts are not facts but hingly influenced by the writers feelings and emotions.\n",
    "\n",
    "We invetigated and detected Text Emotion using Hugging Face Model Hub's EmoRoberta. EmoRoberta leverages Roberta to perform Emotion classification into 28 categories of Emotion. We also explored the NRC Lexicon model which gives the fraction for all 8 emotions to the text.\n",
    "\n",
    "Sentiment Analysis Notebook: [Sentiment Analysis Notebook](https://github.com/tejlibre/dsi-nlp-news/blob/main/Notebooks/Final%20Report/Sentiment%20_%20Emotion%20Analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3XSY2xQTOkv"
   },
   "source": [
    "# 5. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m2BKXw3TDmD"
   },
   "source": [
    "A topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. During this project we chose to apply a Latent Dirichlet Allocation (LDA) technique. LDA topic modelling discovers topics that are hidden (latent) in a set of text documents. It does this by inferring possible topics based on the words in the documents using a generative probabilistic model and Dirichlet distributions. The number of topics is chosen in advance by the user and can be varied using a slider in our deployed app. \n",
    "\n",
    "We tested the LDA models provided by sklearn as well as gensim. We displayed the results using wordcloud visualisations and provided an interactive plot using pyLDAvis.\n",
    "\n",
    "Topic Modeling Notebook: [Topic Modeling Notebook](https://github.com/tejlibre/dsi-nlp-news/blob/main/Notebooks/Final%20Report/Topic_Modeling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0YEwJm-TTch"
   },
   "source": [
    "#6. Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pcr1bJ7DXWkK"
   },
   "source": [
    "Text generation is a subfield of natural language processing (NLP). It leverages knowledge in computational linguistics and artificial intelligence to automatically generate natural language texts, which can satisfy certain communicative requirements. Deep Learning models are trained to generate random but hopefully meaningful text in the simplest form.\n",
    "\n",
    "For this project we pursued 2 approaches:\n",
    "\n",
    "1. Transfer learning: Utilizing a pre-trained model from hugging face to spin articles. The pre-trained model will be fed seed text from a trending subject and will output an article of specified length. \n",
    "2. Training a custom LSTM model on the tweets and predicting a sequence of the next most probable words.\n",
    "\n",
    "**Text generation notebook link**: [Text Generation Notebook](https://github.com/tejlibre/dsi-nlp-news/blob/main/Notebooks/Final%20Report/Text_Generation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzMhT96NTtJz"
   },
   "source": [
    "# 7. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcyosJ06ZhHA"
   },
   "source": [
    "The project's main aim was to design a product (web page) that would act as an aid to a journalist having the following features.\n",
    "\n",
    "- Word cloud visualization on what's trending.\n",
    "- Sentiment and emotion analysis.\n",
    "- Topic discovery leveraging machine learning.\n",
    "- Article generation using deep learning models.\n",
    "\n",
    "A webpage with the above stated features was designed, developed and hosted in the cloud.\n",
    "\n",
    "Some of the tools utilized for the webpage deployment are:\n",
    "\n",
    "- Streamlit: Open-source app framework for Machine Learning and Data Science teams\n",
    "- Streamlit Cloud: Workspace to deploy, share, and collaborate on Streamlit applications.\n",
    "\n",
    "The product name is Scoop Finder.\n",
    "\n",
    "**Web Application link**: [Scoop Finder Link](https://share.streamlit.io/tejlibre/dsi-nlp-news/main/Home_Page/app.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b22hpaVIT0mW"
   },
   "source": [
    "#8. Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNYajBdCJOx-"
   },
   "source": [
    "In this project we were able to create an application that provides the user with a detailed analysis of Twitter data. This app was created using Streamlit and deployed on the cloud using the free serve provided by Streamlit. Below is a detailed summary of the conclusion made for each aspect of the project.\n",
    "\n",
    "Sentiment & Emotion Analysis\n",
    "- Alot of tweets had sentiment analysis scores of neutral. A possible explanation is lamguage barrier. Most probably the transformer and lexicon models had a hard time understanding non-english words.\n",
    "- The emotinal analysis was successfully done using both the Hugging Face transformer Model and the NRC lexicon model.\n",
    "- In our case transformers had a wider range of emotions (28 classes) in comparison to lexicon based (8 classes) and may be preferred for futher analysis or model deployment.\n",
    "- Lexicon based models are super fast in the emotion detection task in comparison to transformers. \n",
    "- With more time, it will be interesting to train transformer model on custom data for both the sentiment and emotion analysis.\n",
    "\n",
    "Topic modelling\n",
    "-\tLDA with initialization of 3 topics had commendable results with less overlap between topics.\n",
    "-\tFurther analysis was also carried out to optimize the model.\n",
    "-\tGiven time we would have considered other algorithms such as Non-Negative matrix factorization.\n",
    "\n",
    "Text Generation\n",
    "-\tBoth transformer model and LSTM model had promising results.\n",
    "-\tWe opted for GPT2 rather than GPT3 due to application response speed. GPT3 performs better than GPT2 but is larger in size and takes longer to load.\n",
    "-\tAn interesting area to pursue given more time is to train transformer model on custom data.\n",
    "- Training the LSTM model for more epochs also looks promising for output improvement.\n",
    "\n",
    "Deployment\n",
    "\n",
    "-\tStreamlit is easy to use, and free online deployment is very useful.\n",
    "-\tCustomisation is somewhat limited.\n",
    "-\tThere are difficulties in resizing pyLDAvis plot.\n",
    "-\tSetting a default colour scheme is important to ensure that all plots and text are clearly visible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXL3jA3BSnM6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IHCaAfdTFVv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
