{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation"
      ],
      "metadata": {
        "id": "1gpgHQI-o4Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "bWdm6G8Er0z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generation is a subfield of natural language processing (NLP). It leverages knowledge in computational linguistics and artificial intelligence to automatically generate natural language texts, which can satisfy certain communicative requirements.\n",
        "Deep Learning (DL) models are trained to generate random but hopefully meaningful text in the simplest form.\n",
        "\n",
        "This script focuses on generating text using 2 approaches and models:\n",
        "1. Transformers - Feeding seed text to a transformer model for text generation (transfer learning).\n",
        "2. Long short-term memory (LSTM) - Training LSTM model on a custom dataset and generating text.\n",
        "\n",
        "This notebook will include the following sections:\n",
        "- Libraries and data importation.\n",
        "- Data cleaning.\n",
        "- Transformers text generation.\n",
        "- LSTM text generation.\n"
      ],
      "metadata": {
        "id": "KWEtPdZuo7dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Import Libraries"
      ],
      "metadata": {
        "id": "KrYCaLlx7wSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hugging Face Transformers functions provide a pool of pre-trained models to perform various tasks such as vision, text, and audio. Transformers provides APIs to download and experiment with the pre-trained models, and we can even fine-tune them on our datasets."
      ],
      "metadata": {
        "id": "FELg2bABuwK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## downloa transformers package from hugging face.\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "Ta5cWSt7-g_o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pHJdeWn87iBJ"
      },
      "outputs": [],
      "source": [
        "## Import libraries\n",
        "import pandas as pd ## data manipulation\n",
        "import re ## For regular expressions\n",
        "import numpy as np ## numerical processing\n",
        "from random import randint ## random number generation\n",
        "from pickle import load ## for serializing and de-serializing\n",
        "import random ## random number generation\n",
        "\n",
        "import spacy ## advanced natural language processing\n",
        "\n",
        "from keras.models import load_model ## loading model\n",
        "from keras.preprocessing.sequence import pad_sequences ## ensure sequences in a list have the same length\n",
        "from keras.preprocessing.text import Tokenizer ## Splitting text to words\n",
        "\n",
        "import keras ## Deep learning framework\n",
        "from keras.models import Sequential ## Sequential model\n",
        "from keras.layers import Dense,LSTM,Embedding ## Import necessary layers\n",
        "from tensorflow.keras.utils import to_categorical ## Converts a class vector (integers) to binary class matrix\n",
        "from pickle import dump,load ## dump and load models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from transformers import pipeline ## Hugging face API"
      ],
      "metadata": {
        "id": "VOrqjV1X-run"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Import Data"
      ],
      "metadata": {
        "id": "tZVfapZv73Hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data harvested from twitter API will be utilized to train the LSTM model. The data is based on trending tweets in Kenya."
      ],
      "metadata": {
        "id": "AhpAIJXYx1JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZsjZTdM70R1",
        "outputId": "cf20974c-4c9a-4a87-c9dd-83c4fca3fd71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import dataset from drive\n",
        "\n",
        "twitter_data = pd.read_csv('/content/drive/MyDrive/NLP/Datasets/Twitter/Location Trend Tweets 2022-04-03.csv')\n",
        "\n",
        "twitter_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2yIu_USD79LU",
        "outputId": "fb1a1c60-2263-484e-ad09-314e5302098f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      screen_name  hashtag                                              tweet  \\\n",
              "0  Gringo42106495  Kondele  RT @ekisiangani: I abhor violence anywhere in ...   \n",
              "1  mwabilimwagodi  Kondele  RT @kipmurkomen: When DP Ruto team was attacke...   \n",
              "2    PeterRatemo4  Kondele  RT @JKNjenga: For the fourth day running, Kond...   \n",
              "3   DavidChirchir  Kondele  RT @NahashonKimemia: President Uhuru Kenyatta ...   \n",
              "4     MabawaYaMbu  Kondele  RT @Silvia_Wangeci: Even After ALL The Violenc...   \n",
              "\n",
              "                  time_stamp  \n",
              "0  2022-04-03 16:01:48+00:00  \n",
              "1  2022-04-03 16:01:43+00:00  \n",
              "2  2022-04-03 16:01:41+00:00  \n",
              "3  2022-04-03 16:01:05+00:00  \n",
              "4  2022-04-03 16:01:01+00:00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8af0ce65-7352-4740-a220-da95d85288f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>screen_name</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>tweet</th>\n",
              "      <th>time_stamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gringo42106495</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @ekisiangani: I abhor violence anywhere in ...</td>\n",
              "      <td>2022-04-03 16:01:48+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mwabilimwagodi</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @kipmurkomen: When DP Ruto team was attacke...</td>\n",
              "      <td>2022-04-03 16:01:43+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PeterRatemo4</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @JKNjenga: For the fourth day running, Kond...</td>\n",
              "      <td>2022-04-03 16:01:41+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DavidChirchir</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @NahashonKimemia: President Uhuru Kenyatta ...</td>\n",
              "      <td>2022-04-03 16:01:05+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MabawaYaMbu</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @Silvia_Wangeci: Even After ALL The Violenc...</td>\n",
              "      <td>2022-04-03 16:01:01+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8af0ce65-7352-4740-a220-da95d85288f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8af0ce65-7352-4740-a220-da95d85288f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8af0ce65-7352-4740-a220-da95d85288f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Clean Data"
      ],
      "metadata": {
        "id": "D4OrmyiJ8I8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A custom data function will be designed to clean the harvested tweets."
      ],
      "metadata": {
        "id": "UGEktaNYyjcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaner (text):\n",
        "  \"\"\" Function to clean text data. \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : A string\n",
        "    \n",
        "  Returns\n",
        "  -------\n",
        "  text : Cleaned string.\n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  text = re.sub(r'@[A-Za-z0-9]+','',str(text)) ## remove @ mentions\n",
        "  text = re.sub(r'#','',str(text)) ## remove # symbol\n",
        "  text = re.sub(r'^RT+','',str(text)) ## remove RT\n",
        "  text = re.sub(r'https?:\\/\\/\\S+','',str(text)) ## remove hyperlink\n",
        "  text = re.sub(r'[^\\w\\s]','',str(text)) ## remove everything apart from words and space\n",
        "  text = re.sub(r'_',' ',str(text)) ## remove underscore\n",
        "  text = re.sub(r'\\n',' ',str(text)) ## remove \\n\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "uODA197h8GDK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create clean text column\n",
        "\n",
        "twitter_data['cleaned_tweet'] = twitter_data['tweet'].apply(text_cleaner)\n",
        "\n",
        "## Select necessary columns\n",
        "twitter_data = twitter_data[['screen_name','hashtag','tweet','cleaned_tweet','time_stamp']]\n",
        "twitter_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XadpsKyj8YgO",
        "outputId": "d0e895ba-7139-4054-9f98-857f22f89e0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      screen_name  hashtag                                              tweet  \\\n",
              "0  Gringo42106495  Kondele  RT @ekisiangani: I abhor violence anywhere in ...   \n",
              "1  mwabilimwagodi  Kondele  RT @kipmurkomen: When DP Ruto team was attacke...   \n",
              "2    PeterRatemo4  Kondele  RT @JKNjenga: For the fourth day running, Kond...   \n",
              "3   DavidChirchir  Kondele  RT @NahashonKimemia: President Uhuru Kenyatta ...   \n",
              "4     MabawaYaMbu  Kondele  RT @Silvia_Wangeci: Even After ALL The Violenc...   \n",
              "\n",
              "                                       cleaned_tweet  \\\n",
              "0    I abhor violence anywhere in the world But l...   \n",
              "1    When DP Ruto team was attacked in Kibera Uhu...   \n",
              "2    For the fourth day running Kondele has remai...   \n",
              "3    President Uhuru Kenyatta has condemned the a...   \n",
              "4    Wangeci Even After ALL The Violence That Was...   \n",
              "\n",
              "                  time_stamp  \n",
              "0  2022-04-03 16:01:48+00:00  \n",
              "1  2022-04-03 16:01:43+00:00  \n",
              "2  2022-04-03 16:01:41+00:00  \n",
              "3  2022-04-03 16:01:05+00:00  \n",
              "4  2022-04-03 16:01:01+00:00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7da9be20-9c3c-4d52-932c-f158dc0fc4ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>screen_name</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "      <th>time_stamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gringo42106495</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @ekisiangani: I abhor violence anywhere in ...</td>\n",
              "      <td>I abhor violence anywhere in the world But l...</td>\n",
              "      <td>2022-04-03 16:01:48+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mwabilimwagodi</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @kipmurkomen: When DP Ruto team was attacke...</td>\n",
              "      <td>When DP Ruto team was attacked in Kibera Uhu...</td>\n",
              "      <td>2022-04-03 16:01:43+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PeterRatemo4</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @JKNjenga: For the fourth day running, Kond...</td>\n",
              "      <td>For the fourth day running Kondele has remai...</td>\n",
              "      <td>2022-04-03 16:01:41+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DavidChirchir</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @NahashonKimemia: President Uhuru Kenyatta ...</td>\n",
              "      <td>President Uhuru Kenyatta has condemned the a...</td>\n",
              "      <td>2022-04-03 16:01:05+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MabawaYaMbu</td>\n",
              "      <td>Kondele</td>\n",
              "      <td>RT @Silvia_Wangeci: Even After ALL The Violenc...</td>\n",
              "      <td>Wangeci Even After ALL The Violence That Was...</td>\n",
              "      <td>2022-04-03 16:01:01+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7da9be20-9c3c-4d52-932c-f158dc0fc4ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7da9be20-9c3c-4d52-932c-f158dc0fc4ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7da9be20-9c3c-4d52-932c-f158dc0fc4ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Text Generation"
      ],
      "metadata": {
        "id": "YUAR1upM3fnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate text we will employ 2 approaches:\n",
        "\n",
        "1. Apply transfer learning. Use a pre-trained model from hugging face API.\n",
        "2. Train a LSTM model on the tweets to generate text."
      ],
      "metadata": {
        "id": "SaPFE8WB3-Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1 Transformers Text Generation"
      ],
      "metadata": {
        "id": "NXGjhfVS_EKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. \n",
        "We will use the pre-trained GPT-Neo 1.3B model from hugging face for this task. GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.\n",
        "GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. The datasets are from academic or professional sources."
      ],
      "metadata": {
        "id": "pXZdcgZ802s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Download gpt3 model from hugging face.\n",
        "%%time\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91Nx43Jj8cE4",
        "outputId": "fd49bda5-1428-4629-dc82-b949c4c93c57"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.9 s, sys: 9.68 s, total: 30.6 s\n",
            "Wall time: 51.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer models requires seed text and minimum length of text to be generated. From data exploration there is alot of tweets on Kenyan election and constitutional reforms. The model will be fed seed text regarding this subject."
      ],
      "metadata": {
        "id": "tpo5XIYs4YXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Feed seed text to a transformer model\n",
        "%%time\n",
        "text = generator(\"Kenya constitution ammendmens\" , do_sample=True, min_length=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAOqJtku_LDW",
        "outputId": "bf22a1f3-8584-4279-a024-ec720c1af1b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.9 s, sys: 188 ms, total: 21.1 s\n",
            "Wall time: 21.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Print text\n",
        "text\n",
        "print(text[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpkckMTo_4IS",
        "outputId": "336b230d-7547-4d77-aa94-106aba16a676"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kenya constitution ammendmens\n",
            "\n",
            "The Constitution of Kenya shall mean the Constitution of Kenya as adopted by the People's Assembly (Chapter I of the Constitution) or as amended by a plebiscite approved as the result of the second reading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model does a decent job attempting to spin a short article on this topic. Based on the fact the model was trained on academic and professional articles such results were expected."
      ],
      "metadata": {
        "id": "-qb6AtzM7NcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2 LSTM Text Generation"
      ],
      "metadata": {
        "id": "zpC7X121BS8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long Short Term Memory Network is an advanced RNN, a sequential network, that allows information to persist. It is capable of handling the vanishing gradient problem faced by RNN. A recurrent neural network is also known as RNN is used for persistent memory.\n",
        "\n",
        "The LSTM will be trained on a sequence of words to predict the next most probable word."
      ],
      "metadata": {
        "id": "P-9DLdRy8x1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 Text Pre-Processing"
      ],
      "metadata": {
        "id": "ybs1Wi_mAo3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load spacy model\n",
        "nlp = spacy.load('en',disable=['parser','tagger','ner'])"
      ],
      "metadata": {
        "id": "dloszkCbAbOu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Remove custom punctuation\n",
        "def separate_punc (doc_text):\n",
        "  \"\"\" Function to remove custom punctuation and tokenize text. \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : A text document\n",
        "    \n",
        "  Returns\n",
        "  -------\n",
        "  text : Tokenized document with custom punctuation removed.\n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
      ],
      "metadata": {
        "id": "miRt7QbMCf5x"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Concatenate tweets to form a document\n",
        "d = twitter_data['cleaned_tweet'].str.cat()"
      ],
      "metadata": {
        "id": "jpJGLHj2EVTX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get document length\n",
        "nlp.max_length = len(d)"
      ],
      "metadata": {
        "id": "Yiaw1e-DGYoF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenize text\n",
        "tokens = separate_punc(d)\n",
        "\n",
        "## Glimpse first few tokens\n",
        "print(len(tokens))\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BsCbK26Ek_S",
        "outputId": "a6fe6ad9-fada-4c09-def4-4b5e44e6248e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42329\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  ',\n",
              " 'i',\n",
              " 'abhor',\n",
              " 'violence',\n",
              " 'anywhere',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'but',\n",
              " 'let']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2.2 Create Sequences"
      ],
      "metadata": {
        "id": "kCE-HIKJGyec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# organize into sequences of tokens\n",
        "train_len = 25+1 # 25 training words , then one target word\n",
        "\n",
        "# Empty list of sequences\n",
        "text_sequences = []\n",
        "\n",
        "## For loop  to show text sequences\n",
        "for i in range(train_len, len(tokens)):\n",
        "    \n",
        "    # Grab train_len# amount of characters\n",
        "    seq = tokens[i-train_len:i]\n",
        "    \n",
        "    # Add to list of sequences\n",
        "    text_sequences.append(seq)"
      ],
      "metadata": {
        "id": "fKiw0k6mF8Od"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## First sequence\n",
        "' '.join(text_sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_zadVc9eGS5A",
        "outputId": "a9400107-d6b4-4fce-9911-12d243efdee6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'   i abhor violence anywhere in the world but let us deal with the problem fairly and without discrimination if violence is when dp ruto team'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Second sequence\n",
        "' '.join(text_sequences[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ivIFnFcHWyG",
        "outputId": "4694f321-a3e9-416f-fcb2-79b3e81793d0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i abhor violence anywhere in the world but let us deal with the problem fairly and without discrimination if violence is when dp ruto team was'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Third sequence\n",
        "' '.join(text_sequences[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "esqZOM4rHaC3",
        "outputId": "5f3617a0-62d7-4356-8ec2-01d3ea71c8ef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abhor violence anywhere in the world but let us deal with the problem fairly and without discrimination if violence is when dp ruto team was attacked'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2.3 Keras Tokenization"
      ],
      "metadata": {
        "id": "O1zcYKPFHpFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text encoding is the process of transforming words into numbers and sequences of words into sequences of numbers. We will first tokenize the text and then convert the text to sequences.\n",
        "To train the text generator model we need to convert the text data into a format digestable by the model."
      ],
      "metadata": {
        "id": "ZN4nb9TnCbc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)"
      ],
      "metadata": {
        "id": "08-7YGFiHhl9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## View first sequence\n",
        "for i in sequences[0]:\n",
        "    print(f'{i} : {tokenizer.index_word[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qcnpf1SIMvw",
        "outputId": "416b8c96-a251-40af-c2e7-3bdc14b9fbca"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 :   \n",
            "16 : i\n",
            "772 : abhor\n",
            "111 : violence\n",
            "697 : anywhere\n",
            "4 : in\n",
            "1 : the\n",
            "347 : world\n",
            "56 : but\n",
            "268 : let\n",
            "43 : us\n",
            "602 : deal\n",
            "24 : with\n",
            "1 : the\n",
            "557 : problem\n",
            "771 : fairly\n",
            "5 : and\n",
            "346 : without\n",
            "769 : discrimination\n",
            "67 : if\n",
            "111 : violence\n",
            "8 : is\n",
            "55 : when\n",
            "49 : dp\n",
            "34 : ruto\n",
            "112 : team\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Get the text vocabulary size\n",
        "vocabulary_size = len(tokenizer.word_counts)\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izgw7MitIbwC",
        "outputId": "06022a6e-aa27-4eb8-b021-c6dac4e5d1c9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5377"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert sequences to numpy array\n",
        "\n",
        "sequences = np.array(sequences)"
      ],
      "metadata": {
        "id": "00j44ArOIjMF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## View resulting array\n",
        "sequences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgmCODmNI5Vb",
        "outputId": "4ab92935-00e7-4a5e-eaec-0524dcbf1d72"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   3,   16,  772, ...,   49,   34,  112],\n",
              "       [  16,  772,  111, ...,   34,  112,   10],\n",
              "       [ 772,  111,  697, ...,  112,   10,   89],\n",
              "       ...,\n",
              "       [  82,   69, 2668, ..., 2671,    2,  603],\n",
              "       [  69, 2668, 2669, ...,    2,  603,  269],\n",
              "       [2668, 2669,   18, ...,  603,  269,   82]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2.4 Create LSTM Model"
      ],
      "metadata": {
        "id": "XxNOIq7VJP65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Split X and y features\n",
        "\n",
        "## All elements in sequence but last element\n",
        "X = sequences[:,:-1]\n",
        "\n",
        "## Last element in sequence\n",
        "y = sequences[:,-1]\n",
        "\n",
        "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
        "\n",
        "seq_len = X.shape[1]\n",
        "\n",
        "seq_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5EztQIEI87A",
        "outputId": "87051ba9-2031-44bf-8bf9-71ff3486c50e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "def create_model(vocabulary_size, seq_len):\n",
        "  \n",
        "  \"\"\" Function to define and compile LSTM model. \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  vocabulary_size : Unique words in text corpus\n",
        "\n",
        "  seq_len : Number of words to use in sequence\n",
        "    \n",
        "  Returns\n",
        "  -------\n",
        "  \n",
        "  model: defined and compiled model\n",
        "  \"\"\"\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
        "  model.add(LSTM(150, return_sequences=True))\n",
        "  model.add(LSTM(150))\n",
        "  model.add(Dense(150, activation='relu'))\n",
        "\n",
        "  model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    \n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "   \n",
        "  model.summary()\n",
        "    \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "titEto1DKRRL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Summary of model\n",
        "model = create_model(vocabulary_size+1, seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3TB3Y0UKu6J",
        "outputId": "a6c2293d-ea17-4a8f-9117-abcdfe27bae0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 25, 25)            134450    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 25, 150)           105600    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 150)               180600    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 150)               22650     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5378)              812078    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,255,378\n",
            "Trainable params: 1,255,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "model.fit(X, y, batch_size=256, epochs=150,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-J1Jg6UKjh4",
        "outputId": "98c40661-62ab-4a0f-d7a1-ac5a2f08fefd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "166/166 [==============================] - 10s 35ms/step - loss: 7.2428 - accuracy: 0.0322\n",
            "Epoch 2/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 6.9049 - accuracy: 0.0340\n",
            "Epoch 3/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 6.7105 - accuracy: 0.0367\n",
            "Epoch 4/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 6.4462 - accuracy: 0.0419\n",
            "Epoch 5/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 6.1715 - accuracy: 0.0498\n",
            "Epoch 6/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 5.9368 - accuracy: 0.0581\n",
            "Epoch 7/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 5.7157 - accuracy: 0.0695\n",
            "Epoch 8/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 5.5006 - accuracy: 0.0838\n",
            "Epoch 9/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 5.3199 - accuracy: 0.0981\n",
            "Epoch 10/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 5.1279 - accuracy: 0.1195\n",
            "Epoch 11/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 4.9401 - accuracy: 0.1430\n",
            "Epoch 12/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 4.7681 - accuracy: 0.1644\n",
            "Epoch 13/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 4.6185 - accuracy: 0.1864\n",
            "Epoch 14/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 4.4840 - accuracy: 0.2038\n",
            "Epoch 15/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 4.3542 - accuracy: 0.2215\n",
            "Epoch 16/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 4.2478 - accuracy: 0.2333\n",
            "Epoch 17/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 4.1368 - accuracy: 0.2476\n",
            "Epoch 18/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 4.0422 - accuracy: 0.2575\n",
            "Epoch 19/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 3.9401 - accuracy: 0.2734\n",
            "Epoch 20/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 3.8491 - accuracy: 0.2866\n",
            "Epoch 21/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 3.7641 - accuracy: 0.2974\n",
            "Epoch 22/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 3.6771 - accuracy: 0.3071\n",
            "Epoch 23/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 3.5985 - accuracy: 0.3187\n",
            "Epoch 24/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 3.5223 - accuracy: 0.3299\n",
            "Epoch 25/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 3.4472 - accuracy: 0.3403\n",
            "Epoch 26/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 3.3743 - accuracy: 0.3516\n",
            "Epoch 27/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 3.3023 - accuracy: 0.3607\n",
            "Epoch 28/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 3.2292 - accuracy: 0.3716\n",
            "Epoch 29/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 3.1680 - accuracy: 0.3778\n",
            "Epoch 30/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 3.0994 - accuracy: 0.3889\n",
            "Epoch 31/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 3.0350 - accuracy: 0.3987\n",
            "Epoch 32/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.9743 - accuracy: 0.4077\n",
            "Epoch 33/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.9251 - accuracy: 0.4147\n",
            "Epoch 34/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.8670 - accuracy: 0.4235\n",
            "Epoch 35/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.8127 - accuracy: 0.4323\n",
            "Epoch 36/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.7689 - accuracy: 0.4404\n",
            "Epoch 37/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.7119 - accuracy: 0.4493\n",
            "Epoch 38/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.6688 - accuracy: 0.4572\n",
            "Epoch 39/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.6189 - accuracy: 0.4641\n",
            "Epoch 40/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.5776 - accuracy: 0.4717\n",
            "Epoch 41/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.5345 - accuracy: 0.4780\n",
            "Epoch 42/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.4944 - accuracy: 0.4883\n",
            "Epoch 43/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.4565 - accuracy: 0.4928\n",
            "Epoch 44/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.4145 - accuracy: 0.5010\n",
            "Epoch 45/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.3803 - accuracy: 0.5075\n",
            "Epoch 46/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.3456 - accuracy: 0.5117\n",
            "Epoch 47/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.3114 - accuracy: 0.5192\n",
            "Epoch 48/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.2795 - accuracy: 0.5252\n",
            "Epoch 49/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.2561 - accuracy: 0.5298\n",
            "Epoch 50/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.2182 - accuracy: 0.5363\n",
            "Epoch 51/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.1889 - accuracy: 0.5420\n",
            "Epoch 52/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.1624 - accuracy: 0.5455\n",
            "Epoch 53/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.1368 - accuracy: 0.5515\n",
            "Epoch 54/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.1093 - accuracy: 0.5568\n",
            "Epoch 55/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.0806 - accuracy: 0.5609\n",
            "Epoch 56/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.0593 - accuracy: 0.5638\n",
            "Epoch 57/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.0356 - accuracy: 0.5690\n",
            "Epoch 58/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.7794 - accuracy: 0.4880\n",
            "Epoch 59/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 4.3879 - accuracy: 0.1802\n",
            "Epoch 60/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.8941 - accuracy: 0.3921\n",
            "Epoch 61/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.5369 - accuracy: 0.4662\n",
            "Epoch 62/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 2.3774 - accuracy: 0.4982\n",
            "Epoch 63/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.2745 - accuracy: 0.5215\n",
            "Epoch 64/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.2186 - accuracy: 0.5311\n",
            "Epoch 65/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.1492 - accuracy: 0.5455\n",
            "Epoch 66/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.0901 - accuracy: 0.5575\n",
            "Epoch 67/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 2.0407 - accuracy: 0.5665\n",
            "Epoch 68/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.9971 - accuracy: 0.5752\n",
            "Epoch 69/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.9690 - accuracy: 0.5810\n",
            "Epoch 70/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.9229 - accuracy: 0.5895\n",
            "Epoch 71/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.8852 - accuracy: 0.5952\n",
            "Epoch 72/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.8518 - accuracy: 0.6037\n",
            "Epoch 73/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.8088 - accuracy: 0.6121\n",
            "Epoch 74/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.7689 - accuracy: 0.6186\n",
            "Epoch 75/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.7362 - accuracy: 0.6255\n",
            "Epoch 76/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.7155 - accuracy: 0.6283\n",
            "Epoch 77/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.6732 - accuracy: 0.6367\n",
            "Epoch 78/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.6392 - accuracy: 0.6403\n",
            "Epoch 79/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.6115 - accuracy: 0.6483\n",
            "Epoch 80/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.5781 - accuracy: 0.6565\n",
            "Epoch 81/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.5482 - accuracy: 0.6591\n",
            "Epoch 82/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.5179 - accuracy: 0.6659\n",
            "Epoch 83/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.4950 - accuracy: 0.6690\n",
            "Epoch 84/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.4580 - accuracy: 0.6772\n",
            "Epoch 85/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.4320 - accuracy: 0.6817\n",
            "Epoch 86/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.4094 - accuracy: 0.6856\n",
            "Epoch 87/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.3860 - accuracy: 0.6904\n",
            "Epoch 88/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.3502 - accuracy: 0.6999\n",
            "Epoch 89/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.3273 - accuracy: 0.7036\n",
            "Epoch 90/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.3073 - accuracy: 0.7049\n",
            "Epoch 91/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.2774 - accuracy: 0.7122\n",
            "Epoch 92/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.2494 - accuracy: 0.7182\n",
            "Epoch 93/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.2216 - accuracy: 0.7237\n",
            "Epoch 94/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.1919 - accuracy: 0.7299\n",
            "Epoch 95/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.1715 - accuracy: 0.7332\n",
            "Epoch 96/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.1476 - accuracy: 0.7387\n",
            "Epoch 97/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.1257 - accuracy: 0.7429\n",
            "Epoch 98/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.1050 - accuracy: 0.7477\n",
            "Epoch 99/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 1.0857 - accuracy: 0.7514\n",
            "Epoch 100/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.0625 - accuracy: 0.7566\n",
            "Epoch 101/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.0424 - accuracy: 0.7587\n",
            "Epoch 102/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 1.0221 - accuracy: 0.7629\n",
            "Epoch 103/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.9953 - accuracy: 0.7701\n",
            "Epoch 104/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.9726 - accuracy: 0.7754\n",
            "Epoch 105/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.9543 - accuracy: 0.7791\n",
            "Epoch 106/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.9292 - accuracy: 0.7833\n",
            "Epoch 107/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.9110 - accuracy: 0.7877\n",
            "Epoch 108/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.8836 - accuracy: 0.7927\n",
            "Epoch 109/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.8668 - accuracy: 0.7991\n",
            "Epoch 110/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.8517 - accuracy: 0.7992\n",
            "Epoch 111/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.8262 - accuracy: 0.8064\n",
            "Epoch 112/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.8059 - accuracy: 0.8110\n",
            "Epoch 113/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.7945 - accuracy: 0.8132\n",
            "Epoch 114/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.7750 - accuracy: 0.8179\n",
            "Epoch 115/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.7537 - accuracy: 0.8234\n",
            "Epoch 116/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.7334 - accuracy: 0.8268\n",
            "Epoch 117/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.7211 - accuracy: 0.8304\n",
            "Epoch 118/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.7055 - accuracy: 0.8319\n",
            "Epoch 119/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.6912 - accuracy: 0.8362\n",
            "Epoch 120/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.6636 - accuracy: 0.8437\n",
            "Epoch 121/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.6503 - accuracy: 0.8451\n",
            "Epoch 122/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.6334 - accuracy: 0.8502\n",
            "Epoch 123/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.6187 - accuracy: 0.8520\n",
            "Epoch 124/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.5954 - accuracy: 0.8583\n",
            "Epoch 125/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.5843 - accuracy: 0.8608\n",
            "Epoch 126/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.5663 - accuracy: 0.8641\n",
            "Epoch 127/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.5510 - accuracy: 0.8680\n",
            "Epoch 128/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.5400 - accuracy: 0.8692\n",
            "Epoch 129/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.5254 - accuracy: 0.8746\n",
            "Epoch 130/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.5167 - accuracy: 0.8744\n",
            "Epoch 131/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.4961 - accuracy: 0.8801\n",
            "Epoch 132/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.4859 - accuracy: 0.8828\n",
            "Epoch 133/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.4735 - accuracy: 0.8852\n",
            "Epoch 134/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.4654 - accuracy: 0.8875\n",
            "Epoch 135/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.4591 - accuracy: 0.8872\n",
            "Epoch 136/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.4337 - accuracy: 0.8953\n",
            "Epoch 137/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.4177 - accuracy: 0.8992\n",
            "Epoch 138/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.4018 - accuracy: 0.9038\n",
            "Epoch 139/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.3890 - accuracy: 0.9078\n",
            "Epoch 140/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.3830 - accuracy: 0.9071\n",
            "Epoch 141/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.3710 - accuracy: 0.9118\n",
            "Epoch 142/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.3585 - accuracy: 0.9138\n",
            "Epoch 143/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.3590 - accuracy: 0.9145\n",
            "Epoch 144/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.3526 - accuracy: 0.9151\n",
            "Epoch 145/150\n",
            "166/166 [==============================] - 6s 35ms/step - loss: 0.3327 - accuracy: 0.9197\n",
            "Epoch 146/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.3204 - accuracy: 0.9231\n",
            "Epoch 147/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.3112 - accuracy: 0.9267\n",
            "Epoch 148/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.3089 - accuracy: 0.9250\n",
            "Epoch 149/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.2989 - accuracy: 0.9280\n",
            "Epoch 150/150\n",
            "166/166 [==============================] - 6s 34ms/step - loss: 0.2897 - accuracy: 0.9309\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f690266dd50>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to file\n",
        "model.save('/content/drive/MyDrive/Module 3/Outputs/epochBIG.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('/content/drive/MyDrive/Module 3/Outputs/epochBIG', 'wb'))"
      ],
      "metadata": {
        "id": "QH5utA4IK3Vl"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2.5 Generate Text"
      ],
      "metadata": {
        "id": "v5ooM2voxiTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
        "  ''' Parameters\n",
        "    ----------\n",
        "\n",
        "  model : model that was trained on text data\n",
        "  tokenizer : tokenizer that was fit on text data\n",
        "  seq_len : length of training sequence\n",
        "  seed_text : raw string text to serve as the seed\n",
        "  num_gen_words : number of words to be generated by model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  \n",
        "  generated text: generated text of specified length\n",
        "\n",
        "\n",
        "  '''\n",
        "    \n",
        "  # Final Output\n",
        "  output_text = []\n",
        "    \n",
        "  # Intial Seed Sequence\n",
        "  input_text = seed_text\n",
        "    \n",
        "  # Create num_gen_words\n",
        "  for i in range(num_gen_words):\n",
        "        \n",
        "      # Take the input text string and encode it to a sequence\n",
        "      encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        \n",
        "      # Pad sequences to our trained rate (50 words in the video)\n",
        "      pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "        \n",
        "      # Predict Class Probabilities for each word\n",
        "      pred_word_ind = np.argmax(model.predict(pad_encoded), axis=-1)[0]\n",
        "\n",
        "        \n",
        "      # Grab word\n",
        "      pred_word = tokenizer.index_word[pred_word_ind] \n",
        "        \n",
        "      # Update the sequence of input text (shifting one over with the new word)\n",
        "      input_text += ' ' + pred_word\n",
        "        \n",
        "      output_text.append(pred_word)\n",
        "        \n",
        "  # Make it look like a sentence.\n",
        "  return ' '.join(output_text)"
      ],
      "metadata": {
        "id": "7mQ0_kdvGm-_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Randomly select 1 sequence\n",
        "random.seed(101)\n",
        "random_pick = random.randint(0,len(text_sequences))"
      ],
      "metadata": {
        "id": "wrOsb7TZypPQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## View selected sequence\n",
        "random_seed_text = text_sequences[random_pick]\n",
        "random_seed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IaDSwgb0V2z",
        "outputId": "ccf03138-7da4-4dae-a163-76f87321ab5d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you',\n",
              " 'against',\n",
              " 'other',\n",
              " 'communities',\n",
              " 'desist',\n",
              " 'from',\n",
              " 'overreactingm',\n",
              " 'those',\n",
              " 'are',\n",
              " 'kalenjins',\n",
              " 'killing',\n",
              " 'each',\n",
              " 'other',\n",
              " 'because',\n",
              " 'of',\n",
              " 'there',\n",
              " 'selfish',\n",
              " 'interest',\n",
              " 'not',\n",
              " 'about',\n",
              " 'high',\n",
              " 'cost',\n",
              " 'of',\n",
              " 'living',\n",
              " 'wacha',\n",
              " 'upuzi']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Join seed text\n",
        "seed_text = ' '.join(random_seed_text)\n",
        "seed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vLu85-Is0meg",
        "outputId": "b5d4de68-58dd-4464-a0b0-cd323eb26830"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you against other communities desist from overreactingm those are kalenjins killing each other because of there selfish interest not about high cost of living wacha upuzi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load saved model\n",
        "model = load_model('/content/drive/MyDrive/Module 3/Outputs/epochBIG.h5')"
      ],
      "metadata": {
        "id": "foyJ1Sjh0usR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate next 10 words from sequence\n",
        "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CJ26W9OY1ZqS",
        "outputId": "3479e6d2-c55b-4fa7-f3d9-ed8d66c82f84"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'brian dear kalenjinsbe very carefulsomebody is working hard to turn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a training of 150 epochs the LSTM model has some commendable performance. The model attains ~ 93% accuracy. The model predicts a sequence of sensible words as output."
      ],
      "metadata": {
        "id": "bZ9Vs4SIQC_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "s_ZGl6HlP72I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Both transformer model and LSTM model had promising results.\n",
        "- An interesting area to pursue given more time is to train transformer model on custom data.\n",
        "- Training the LSTM model for more epochs also looks promising for output improvement."
      ],
      "metadata": {
        "id": "6g1G587cQt26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IxJPCS8C1lzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8qO_f1KD3ENS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PL_H9NXM3GcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}